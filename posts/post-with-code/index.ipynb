{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Mapping the Mind's Meanderings: Using NLP to Identify Topic Shifts in Spontaneous Thought\"\n",
        "author: \"Faustine Corbani\"\n",
        "date: \"2024-05-11\"\n",
        "categories: [news, code, analysis]\n",
        "image: \"image.jpg\"\n",
        "---"
      ],
      "id": "0f7f8552"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our minds rarely sit still. Whether we're walking, eating, working, or socializing, our minds are continuously roving through a mental landscape of thoughts, memories, plans, fantasies, and reflections. Studies suggest that we spend between 30-50% of our waking hours engaging in these spontaneous thoughts.\n",
        "\n",
        "Given the ubiquity of spontaneous thought, it's essential that we develop tools to quantify and analyze people's streams of thoughts. In this blog post, I will detail one such tool: how we can leverage natural language processing (NLP) and topic modeling to identify topic jumps within a transcribed stream of thoughts. Prior research has observed a \"clump-and-jump\" structure in thought, where thoughts cluster around a specific theme before transitioning, at times abruptly, to a new, unrelated topic. Prior work has relied on human annotation to identify instances when thoughts shift from one topic to the next. However, this method is unfeasible in large datasets. This blog post will detail how advancements in natural language processing (NLP) and topic modeling can automate the detection of these transitions. Specifically, in this tutorial, I will use Python to apply clustering algorithms to segment thoughts extracted from verbal data to visualize and analyze the dynamics of spontaneous thought more efficiently.\n",
        "\n",
        "In this tutorial, I will be using a dataset of 746 subjects who were were instructed to narrate their stream of thought in real time, saying whatever is going through your mind from moment to moment for 10 minutes. The result is a dataframe where each row corresponds to a subject and there is a column called 'sentences' which contains a list of strings, where each string corresponds to a thought.\n",
        "\n",
        "## Set-Up\n",
        "\n",
        "First, you need to load the following packages & your data.\n"
      ],
      "id": "9e783ff6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "import ast\n",
        "#import umap"
      ],
      "id": "de0aaf6f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load data\n",
        "data = pd.read_csv(\"/Users/faustinecorbani/Desktop/emotion_thought/data/semi-clean/subject_level_dynamic_measures.csv\")\n"
      ],
      "id": "368b49d0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "\n",
        "# Get the first 30 unique subject_ids\n",
        "unique_subject_ids = data['subject_id'].unique()[:30]\n",
        "\n",
        "# Select rows where subject_id is in the list of 30 unique subject_ids\n",
        "data = data[data['subject_id'].isin(unique_subject_ids)]\n",
        "\n",
        "data['sentences'] = data['sentences'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])\n"
      ],
      "id": "2e8bfdcd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Assuming 'data' is your DataFrame\n",
        "print(tabulate(data.head(), headers='keys', tablefmt='psql'))\n"
      ],
      "id": "0dc05db3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Understanding Sentence Transformers and Embeddings**\n",
        "\n",
        "A principal question in NLP is how to represent textual data in a format that computers can understand and work with easily. One solution is to convert language to numerical data through sentence embeddings, which encode pieces of text as high-dimensional vectors. These vectors capture meaningful semantic information, where sentences that are more similar to one another share more similar representations. This property is crucial for clustering and identifying relationships in large text corpora.\n",
        "\n",
        "One popular approach to create sentence embeddings is to use pre-trained models that are able to take text as an input and output a vector corresponding to a sentence embedding. In this tutorial, I will use the pre-trained ‘all-mpnet-base-v2’ model from Hugging Face’s sentence-transformers library that maps sentences to a 768 dimensional dense vector space. I chose this model because it provides the best quality sentence embeddings per the platform's model comparison and evaluation criteria (<https://www.sbert.net/docs/pretrained_models.html>)\n",
        "\n",
        "Thus, our first step is to load the model:\n"
      ],
      "id": "55ce7570"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Initialize the sentence transformer model\n",
        "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')"
      ],
      "id": "9b876b54",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Dimensionality Reduction**\n",
        "\n",
        "### What is the curse of dimensionality?\n",
        "\n",
        "The \"curse of dimensionality\" is a term often used within the machine learning literature to describe a challenges that emerge when working with data in high-dimensional spaces. As you add more dimensions, the space within which data points exist increases exponentially. A helpful analogy is to think of going from a line, to a square, to a cube: as each new dimension is added, you need increasingly more points to cover the space, results in more empty area. Therefore, as mentioned, with more dimensions, data becomes sparse, meaning that most of the high-dimensional space is empty. This makes clustering and classification tasks challenging. In addition, more dimensions require more computing power and time to process the data, and there is a risk of overfitting, where models might start to \"learn\" the noise in the data instead of the actual patterns, which can mislead predictions or classifications.\n",
        "\n",
        "To overcome these issues, we can use dimensionality reduction techniques that simplify the complexity of high-dimensional data while preserving its essential features. One such method is UMAP: Uniform Manifold Approximation and Projection. By mapping high-dimensional sentence embeddings to a two-dimensional space, UMAP helps us visually discern patterns and clusters in the data, indicating how thoughts might cluster around themes or make abrupt jumps to different topics.\n",
        "\n",
        "Next, we want to extract our sentence embeddings for each sentence.\n"
      ],
      "id": "d1a4e70a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Function to get embeddings\n",
        "def get_embeddings(sentences_list):\n",
        "    return model.encode(sentences_list)\n",
        "\n",
        "# Apply UMAP to each subject and collect results\n",
        "all_umap_embeddings = []\n",
        "subject_ids = []\n",
        "\n",
        "def process_subject_data(subject_data):\n",
        "    subject_sentences = [sentence for sublist in subject_data['sentences'] for sentence in sublist]\n",
        "    if not subject_sentences:\n",
        "        return None  # Return None if there are no sentences\n",
        "    embeddings = get_embeddings(subject_sentences)\n",
        "    umap_reducer = umap.UMAP()  # UMAP instance\n",
        "    return umap_reducer.fit_transform(embeddings)"
      ],
      "id": "63759043",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "naturalistic",
      "language": "python",
      "display_name": "naturalistic"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}